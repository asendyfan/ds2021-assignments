{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv7g6EQlWZGr"
      },
      "source": [
        "# <center>Algorithms assignment 1 </center>#\n",
        "\n",
        "## Steps\n",
        "\n",
        "### Step 1: Data Prepare\n",
        "\n",
        "this step is for reading data from csv, and scaling the features.\n",
        "\n",
        "### Step 2: Linear classifier\n",
        "\n",
        "* using logistic gradient descent to get optimal thetas, and the value of thetas is below, in the realised function, I use the $\\lambda = 0$ as hyperparameters for regulations:\n",
        "\n",
        " $$\\theta_{0} = 1.29, \\theta_{1} = 13.47, \\theta_{2} = 7.14 $$\n",
        "\n",
        "* using encapsulated function to calculate cost function and accuracy, and the answer is\n",
        "\n",
        "  $$J(\\theta) = 0.21, Accuracy = 0.91$$\n",
        "\n",
        "* the incorrect indexes: $[0, 11, 12, 22, 38, 58, 79, 85, 98]$\n",
        "\n",
        "\n",
        "### Step 3: Quadratic classifier\n",
        "\n",
        "Since it is quadratic, it totally has 6 thetas, which regression can be generate in this way:\n",
        "\n",
        "$$f(x_{1}, x_{2}) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} +  \\theta_{3}x_{1}x_{2} + \\theta_{4}x_{1}^2 + \\theta_{5}x_{2}^2 $$\n",
        "\n",
        "1. add extra features columns to fix this method, and then traing the data. The answer is:\n",
        "\n",
        "$$\\vec{\\theta} = [1.76, 13.28, 7.53, 1.47, -3.14, -3.45]$$\n",
        "\n",
        "2. after modeling, the cost function $J(\\theta)$ and accuracy is\n",
        "$$J(\\theta) = 0.18, Accuracy = 0.94$$\n",
        "\n",
        "3. the incorrect indexes: $[11, 12, 58, 79, 85, 98]$\n",
        "\n",
        "\n",
        "### Step 4: Naive Bayes classifier\n",
        "\n",
        "1. the mean and standard deviation of each feature for each class:\n",
        "\n",
        "|(Mean, StandardDeviation)|TumourSize|Age|\n",
        "|---|---|---|\n",
        "|Cancer|(4.8, 1.72)|(69.03, 18.84)|\n",
        "|$\\neg $Cancer|(1.66, 1.4) |(62.46, 21.99) |\n",
        "\n",
        "2. using naive bayes formulate to predict. And the formulate and interpretation behind the code is below:\n",
        "\n",
        "  $$P(C|t, a) = \\frac{P(t, a|C)P(C)}{P(t, a)}$$\n",
        "\n",
        "  where\n",
        "  $$P(t, a) = P(t, a|C)|P(C) + P(t, a|\\neg C)P(\\neg C)$$\n",
        "\n",
        "  and using naive bayes, we can split $P(t, a|C)$ and P(t, a|\\neg C) to the multiplication of each one single event:\n",
        "\n",
        "  $$P(t, a|C)=P(t|C)P(a|C)$$\n",
        "  $$P(t, a|\\neg C) = P(t|\\neg C)P(a|\\neg C)$$\n",
        "\n",
        "  Since both of features are numeric values, we can assume that it follow **the Gaussian Distribution**.\n",
        "\n",
        "  After using above principle and formulate, the result is:\n",
        "  $$Accuracy = 0.84$$\n",
        "\n",
        "3. the incorrect indexes: $[0, 4, 11, 12, 19, 50, 58, 59, 72, 73, 84, 85, 87, 91, 96, 98]$\n",
        "\n",
        "\n",
        "### Question 4: Process of Model Selection\n",
        "\n",
        "I would like to follow these steps to find the best model from these three models:\n",
        "1. **Using cross-validation** and **tuning the hyperparameters** for each model to find **their own highest average accuracy value**.\n",
        "2. Then choose the model which has **the highest average accuracy**.\n",
        "3. **Retrain** the model on **all the data** to get **the best parameters**.\n",
        "\n",
        "#### step 1: cross-validation and hyperparameters tuning\n",
        "\n",
        "Using cross-validation is to eliminate the bias on the data, which means the test data may be suitable for one of the models, but not suitable for others, it will cause overfitting and underfitting.\n",
        "And the method we can use is **K-fold Cross-Validation** or **Leave-One-Out Cross-Validation** to finish it.\n",
        "\n",
        "* About the K-fold Cross-Validation, its process is:\n",
        "  1. Reshuffle the dataset\n",
        "  2. Split the dataset into number K subsets\n",
        "  3. Train the K-1 subsets, and the other one use for testing, from testing we can get metric value (accuracy)\n",
        "  4. Repeat the last step K times, but each time testing set is difference. It means each subset will be tested and using other subsets to train.\n",
        "  5. Get the average metric value (accuracy).\n",
        "\n",
        "* Or using Leave-One_Out cross-validation: it is similar to k-fold, but the split size is only 1.\n",
        "\n",
        "And during step 1, we also need to **tune each model hyperparameters**, because different hyperparameters in the same model also have different performances. like the regularisation$\\lambda$, learning rate \\alpha in logistic regression. We can use **Random Search** and **Exhaustive Grid Search** based on cross-validation to do it. We can do it in the following approach.\n",
        "\n",
        "1. Firstly, use the Random Search method to find the good ranges of hyperparameters and check if the cost change significantly around good values.\n",
        "2. Then use Exhaustive Grid Search to iterate each hyperparameter during these good ranges, so that it will get the best group of hyperparameters that have the highest accuracy.\n",
        "\n",
        "Finally, we can find the best hyperparameter for each model, and after cross-validation, we can get its average accuracy.\n",
        "\n",
        "#### step 2\n",
        "Having average accuracy for each model, we can compare it so that we can know which model is the final choice.\n",
        "\n",
        "#### step 3\n",
        "When we have selected the model to be used, we also need to get the model best parameters value. This time, we **retrain all the data** to get it with the best hyperparameter value. And the final model function will be used to predict the new data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4eWGjtar2qo"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eKCL6I_mgND"
      },
      "outputs": [],
      "source": [
        "# step 1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# read data\n",
        "data = pd.read_csv('Data_CW.csv')\n",
        "columns = data.columns\n",
        "data = data.to_numpy() # change data to numpy\n",
        "\n",
        "def feature_scaling(data, pre_data=None):\n",
        "  '''\n",
        "    Param data: 2-D ndarray\n",
        "    Param pre_data: null or 2-D ndarray\n",
        "  '''\n",
        "  n = len(data[0])\n",
        "  changedData = np.array([[] for i in range(len(data))])\n",
        "  for i in range(n):\n",
        "    column = data[:, i]\n",
        "    cal_column = column if (pre_data is None) else pre_data[:, i]\n",
        "    scaling_column = ((column - np.average(cal_column)) / (np.max(cal_column) - np.min(cal_column))).reshape(-1, 1)\n",
        "    changedData = np.c_[changedData, scaling_column]\n",
        "  return changedData\n",
        "\n",
        "X = feature_scaling(data[:, :2])# scale the features\n",
        "y = data[:, 2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3CFgpw791Ff"
      },
      "outputs": [],
      "source": [
        "# this section encapsulate some logistic regression function\n",
        "def logistic_batch_gradient_descent(X, y, eta, iterations, lam=0):\n",
        "  \"\"\"\"using regularise batch gradient descent to calculte the theta\n",
        "    Param X: Metrics\n",
        "    Param y: metrics\n",
        "    Param eta: learning rate\n",
        "    Param iterations: iterate times\n",
        "    Param lam: regularisation param\n",
        "\n",
        "    Return the optimal theta\n",
        "  \"\"\"\n",
        "  m = len(X)\n",
        "  X_b = np.c_[np.ones((m, 1)), X]\n",
        "  n = len(X_b[0])\n",
        "  theta = np.zeros((n, 1))\n",
        "  for i in range(iterations):\n",
        "    t = X_b.dot(theta)\n",
        "    SIG = 1 / (1 + np.exp(-t))\n",
        "    regularisation = lam * np.array([[0], *theta[1:]])\n",
        "    theta = theta - eta*(1/m) * (X_b.T.dot(SIG - y) + regularisation )\n",
        "  return theta\n",
        "\n",
        "def logistic_predict_value(X, theta):\n",
        "  X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "  t = X_b.dot(theta)\n",
        "  SIG = 1 / (1 + np.exp(-t))\n",
        "  return SIG\n",
        "\n",
        "def logistic_cost_function(X, y, theta):\n",
        "  \"\"\"calculate cost function\n",
        "  Param X: 2-D ndarray\n",
        "  Param y: 2-D shape:(n, 1) ndarray\n",
        "  Param theta: 2-D ndarray\n",
        "  \n",
        "  Return: the value of the cost function, accuracy\n",
        "  \"\"\"\n",
        "  SIG = logistic_predict_value(X, theta)\n",
        "  m = len(X)\n",
        "  J = -1/m * (sum(y.T.dot(np.log(SIG)) + (1 - y).T.dot(np.log(1-SIG))))\n",
        "  return J[0]\n",
        "\n",
        "def logistic_accuracy(y, SIG):\n",
        "  \"\"\"\n",
        "  Param y: actual value\n",
        "  Param SIG: predicted value\n",
        "  Return accuracy\n",
        "  \"\"\"\n",
        "  trueValue = ((SIG + y) < 0.5) | ((SIG + y) >= 1.5)\n",
        "  accuracy = len(SIG[trueValue])/len(SIG)\n",
        "  return accuracy\n",
        "\n",
        "def incorrect_prediction_indexes(y, SIG):\n",
        "  predict = np.round(SIG).astype(np.int)\n",
        "  # print(np.c_[y, predict, y!=predict])\n",
        "  y_df = pd.DataFrame(y)\n",
        "  predict_df = pd.DataFrame(predict)\n",
        "  return y_df[predict_df!=y_df].dropna().index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBo34rIRs_EY",
        "outputId": "60b2aec5-edca-425e-93d4-3cd0d9d7c421"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Logistic Regression\n",
            "thetas: [1.29, 13.47, 7.14]\n",
            "cost: 0.21, accuracy: 0.91\n",
            "incorrect indexes: [0, 11, 12, 22, 38, 58, 79, 85, 98]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = .1\n",
        "iterations = 10000\n",
        "\n",
        "# linear classifier\n",
        "print('Linear Logistic Regression')\n",
        "# Step 2.1 caculate thetas\n",
        "thetas = logistic_batch_gradient_descent(X, y, learning_rate, iterations, lam=0)\n",
        "print('thetas:', [round(v, 2) for v in thetas.reshape(3)])\n",
        "\n",
        "# step 2.2 get cost and accuracy\n",
        "cost = logistic_cost_function(X, y, thetas)\n",
        "predictedValue = logistic_predict_value(X, thetas)\n",
        "accuracy = logistic_accuracy(y, predictedValue)\n",
        "print('cost: {}, accuracy: {}'.format(round(cost, 2), accuracy))\n",
        "\n",
        "# step 2.3 error indexes\n",
        "# incorrect_prediction_indexes(y, predictedValue)\n",
        "indexes = incorrect_prediction_indexes(y, predictedValue)\n",
        "print('incorrect indexes:', indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJBcMJ71PrHu",
        "outputId": "ce2f222d-e02f-4086-e307-4b2bcdf5c346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quadratic classifier\n",
            "thetas [1.76, 13.28, 7.53, 1.47, -3.14, -3.45]\n",
            "cost: 0.18, accuracy: 0.94\n",
            "incorrect indexes: [11, 12, 58, 79, 85, 98]\n"
          ]
        }
      ],
      "source": [
        "# step 3\n",
        "def getQuadraticFeatures(features):\n",
        "  '''\n",
        "  Param features (n*2 matrix, type:ndarray)\n",
        "  return n*5 matrix, type:ndarray\n",
        "  '''\n",
        "  quadraticFeatures = []\n",
        "  for x1, x2 in features:\n",
        "    item = [x1, x2, x1*x2, x1**2, x2**2]\n",
        "    quadraticFeatures.append(item)\n",
        "  return np.array(quadraticFeatures)\n",
        "\n",
        "quadraticX = getQuadraticFeatures(X)\n",
        "\n",
        "print('Quadratic classifier')\n",
        "# step 3.1\n",
        "quadraticTheta = logistic_batch_gradient_descent(quadraticX, y, learning_rate, iterations, lam=0)\n",
        "print('thetas', [round(v, 2) for v in quadraticTheta.reshape(6)])\n",
        "# step 3.2\n",
        "quadraticCost = logistic_cost_function(quadraticX, y, quadraticTheta)\n",
        "quadPredictedValue = logistic_predict_value(quadraticX, quadraticTheta)\n",
        "quadraticAccuracy = logistic_accuracy(y, quadPredictedValue)\n",
        "print('cost: {}, accuracy: {}'.format(round(quadraticCost, 2), quadraticAccuracy))\n",
        "# step 3.3\n",
        "quadIndexes = incorrect_prediction_indexes(y, quadPredictedValue)\n",
        "print('incorrect indexes:', quadIndexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkkulES-coiC",
        "outputId": "aeb984ab-8fe2-4668-b236-5bb9167bfc63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P_tumourGivenCancer mean: 4.8, standard deviation:1.71\n",
            "P_ageGivenCancer mean: 69.03, standard deviation:18.84\n",
            "P_tumourGivenNoCancer mean: 1.66, standard deviation:1.4\n",
            "P_ageGivenNoCancer mean: 62.46, standard deviation:21.99\n",
            "accuracy: 0.84\n",
            "incorrect indexes: [0, 4, 11, 12, 19, 50, 58, 59, 72, 73, 84, 85, 87, 91, 96, 98]\n"
          ]
        }
      ],
      "source": [
        "# step 4\n",
        "from scipy import stats\n",
        "\n",
        "tumourSizeData = data[:, 0]\n",
        "ageData = data[:, 1]\n",
        "cancer = data[:, 2]\n",
        "tumourSizeMean = tumourSizeData.mean()\n",
        "tumourSizeStd = tumourSizeData.std()\n",
        "\n",
        "ageMean = ageData.mean()\n",
        "ageStd = ageData.std()\n",
        "\n",
        "tumourGivenCancerData = tumourSizeData[cancer == 1]\n",
        "ageGivenCancerData = ageData[cancer == 1]\n",
        "tumourGivenNoCancerData = tumourSizeData[cancer == 0]\n",
        "ageGivenNoCancerData = ageData[cancer == 0]\n",
        "def var_name(var, all_vars = locals()):\n",
        "  result = [var_name for var_name in all_vars if all_vars[var_name] is var]\n",
        "  return result[0] \n",
        "\n",
        "# print each class mean and standard deviation\n",
        "for specificData in [tumourGivenCancerData, ageGivenCancerData, tumourGivenNoCancerData, ageGivenNoCancerData]:\n",
        "    mean = specificData.mean()\n",
        "    std = specificData.std()\n",
        "    name = var_name(specificData).replace('Data','')\n",
        "    print('P_{} mean: {}, standard deviation:{}'.format(name, round(mean, 2), round(std, 2)))\n",
        "\n",
        "# using Gaussian Distribution to calculate the specific data\n",
        "def calOneColumnProbability(value, specificData):\n",
        "  mean = specificData.mean()\n",
        "  std = specificData.std()\n",
        "  probability = stats.norm.pdf(value, mean, std)\n",
        "  return probability\n",
        "\n",
        "\n",
        "# the probability of having specific tumour size and age, given patients having cancer\n",
        "def calP_TumourAgeGivenCancer(size, age):\n",
        "  P_TumourGivenCancer = calOneColumnProbability(size, tumourGivenCancerData)\n",
        "  P_AgeGivenCancer = calOneColumnProbability(age, ageGivenCancerData)\n",
        "  return P_TumourGivenCancer * P_AgeGivenCancer\n",
        "\n",
        "\n",
        "# the probability of having specific tumour size and age, given patients not having cancer\n",
        "def calP_TumourAgeGivenNoCancer(size, age):\n",
        "  P_tumourGivenNoCancer = calOneColumnProbability(size, tumourGivenNoCancerData)\n",
        "  P_ageGivenNoCancer = calOneColumnProbability(age, ageGivenNoCancerData)\n",
        "  return P_tumourGivenNoCancer * P_ageGivenNoCancer\n",
        "\n",
        "\n",
        "# the probability of having cancer given tumour size and age\n",
        "def calP_CancerGivenTumourAge(size, age):\n",
        "  P_Cancer = len(cancer[cancer == 1])/len(cancer)\n",
        "  P_NoCancer = 1 - P_Cancer\n",
        "  P_TumourAgeGivenCancer = calP_TumourAgeGivenCancer(size, age)\n",
        "  P_TumourAgeGivenNoCancer = calP_TumourAgeGivenNoCancer(size, age)\n",
        "  return P_TumourAgeGivenCancer * P_Cancer / (P_TumourAgeGivenCancer * P_Cancer + P_TumourAgeGivenNoCancer * P_NoCancer)\n",
        "\n",
        "# calculate accuracy\n",
        "def cal_accuracy_and_incorrect_indexes():\n",
        "  right_num = 0\n",
        "  incorrect_indexes = []\n",
        "  for i, item in enumerate(data):\n",
        "    probability = calP_CancerGivenTumourAge(item[0], item[1])\n",
        "    if (probability > .5 and item[2] == 1) or (probability < .5 and item[2] == 0):\n",
        "      right_num += 1\n",
        "    else:\n",
        "      incorrect_indexes.append(i)\n",
        "  return right_num / len(data), incorrect_indexes\n",
        "\n",
        "bayesAccuracy, incorrect_indexes = cal_accuracy_and_incorrect_indexes()\n",
        "print('accuracy:',bayesAccuracy)\n",
        "print('incorrect indexes:', incorrect_indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKQTabjseT6W"
      },
      "outputs": [],
      "source": [
        "# step 1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# read data\n",
        "data = pd.read_csv('Data_CW.csv')\n",
        "columns = data.columns\n",
        "data = data.to_numpy() # change data to numpy\n",
        "\n",
        "def feature_scaling(data, pre_data=None):\n",
        "  '''\n",
        "    Param data: 2-D ndarray\n",
        "    Param pre_data: null or 2-D ndarray\n",
        "  '''\n",
        "  n = len(data[0])\n",
        "  changedData = np.array([[] for i in range(len(data))])\n",
        "  for i in range(n):\n",
        "    column = data[:, i]\n",
        "    cal_column = column if (pre_data is None) else pre_data[:, i]\n",
        "    scaling_column = ((column - np.average(cal_column)) / (np.max(cal_column) - np.min(cal_column))).reshape(-1, 1)\n",
        "    changedData = np.c_[changedData, scaling_column]\n",
        "  return changedData\n",
        "\n",
        "X = feature_scaling(data[:, :2])# scale the features\n",
        "y = data[:, 2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu8pWgpIfNlQ"
      },
      "outputs": [],
      "source": [
        "# this section encapsulate some logistic regression function\n",
        "def logistic_batch_gradient_descent(X, y, eta, iterations, lam=0):\n",
        "  \"\"\"\"using regularise batch gradient descent to calculte the theta\n",
        "    Param X: Metrics\n",
        "    Param y: metrics\n",
        "    Param eta: learning rate\n",
        "    Param iterations: iterate times\n",
        "    Param lam: regularisation param\n",
        "\n",
        "    Return the optimal theta\n",
        "  \"\"\"\n",
        "  m = len(X)\n",
        "  X_b = np.c_[np.ones((m, 1)), X]\n",
        "  n = len(X_b[0])\n",
        "  theta = np.zeros((n, 1))\n",
        "  for i in range(iterations):\n",
        "    t = X_b.dot(theta)\n",
        "    SIG = 1 / (1 + np.exp(-t))\n",
        "    regularisation = lam * np.array([[0], *theta[1:]])\n",
        "    theta = theta - eta*(1/m) * (X_b.T.dot(SIG - y) + regularisation )\n",
        "  return theta\n",
        "\n",
        "def logistic_predict_value(X, theta):\n",
        "  X_b = np.c_[np.ones((len(X), 1)), X]\n",
        "  t = X_b.dot(theta)\n",
        "  SIG = 1 / (1 + np.exp(-t))\n",
        "  return SIG\n",
        "\n",
        "def logistic_cost_function(X, y, theta):\n",
        "  \"\"\"calculate cost function\n",
        "  Param X: 2-D ndarray\n",
        "  Param y: 2-D shape:(n, 1) ndarray\n",
        "  Param theta: 2-D ndarray\n",
        "  \n",
        "  Return: the value of the cost function, accuracy\n",
        "  \"\"\"\n",
        "  SIG = logistic_predict_value(X, theta)\n",
        "  m = len(X)\n",
        "  J = -1/m * (sum(y.T.dot(np.log(SIG)) + (1 - y).T.dot(np.log(1-SIG))))\n",
        "  return J[0]\n",
        "\n",
        "def logistic_accuracy(y, SIG):\n",
        "  \"\"\"\n",
        "  Param y: actual value\n",
        "  Param SIG: predicted value\n",
        "  Return accuracy\n",
        "  \"\"\"\n",
        "  trueValue = ((SIG + y) < 0.5) | ((SIG + y) >= 1.5)\n",
        "  accuracy = len(SIG[trueValue])/len(SIG)\n",
        "  return accuracy\n",
        "\n",
        "def incorrect_prediction_indexes(y, SIG):\n",
        "  predict = np.round(SIG).astype(np.int)\n",
        "  # print(np.c_[y, predict, y!=predict])\n",
        "  y_df = pd.DataFrame(y)\n",
        "  predict_df = pd.DataFrame(predict)\n",
        "  return y_df[predict_df!=y_df].dropna().index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fuvdr7KfPGK",
        "outputId": "fb57a155-ea3b-4f14-aedb-4d05216f3e9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Logistic Regression\n",
            "thetas: [1.29, 13.47, 7.14]\n",
            "cost: 0.21, accuracy: 0.91\n",
            "incorrect indexes: [0, 11, 12, 22, 38, 58, 79, 85, 98]\n"
          ]
        }
      ],
      "source": [
        "learning_rate = .1\n",
        "iterations = 10000\n",
        "\n",
        "# linear classifier\n",
        "print('Linear Logistic Regression')\n",
        "# Step 2.1 caculate thetas\n",
        "thetas = logistic_batch_gradient_descent(X, y, learning_rate, iterations, lam=0)\n",
        "print('thetas:', [round(v, 2) for v in thetas.reshape(3)])\n",
        "\n",
        "# step 2.2 get cost and accuracy\n",
        "cost = logistic_cost_function(X, y, thetas)\n",
        "predictedValue = logistic_predict_value(X, thetas)\n",
        "accuracy = logistic_accuracy(y, predictedValue)\n",
        "print('cost: {}, accuracy: {}'.format(round(cost, 2), accuracy))\n",
        "\n",
        "# step 2.3 error indexes\n",
        "# incorrect_prediction_indexes(y, predictedValue)\n",
        "indexes = incorrect_prediction_indexes(y, predictedValue)\n",
        "print('incorrect indexes:', indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYjHjCMwfT4o",
        "outputId": "6ddcb0c7-de97-4ead-afdc-261a5e4850e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P_tumourGivenCancer mean: 4.8, standard deviation:1.71\n",
            "P_ageGivenCancer mean: 69.03, standard deviation:18.84\n",
            "P_tumourGivenNoCancer mean: 1.66, standard deviation:1.4\n",
            "P_ageGivenNoCancer mean: 62.46, standard deviation:21.99\n",
            "accuracy: 0.84\n",
            "incorrect indexes: [0, 4, 11, 12, 19, 50, 58, 59, 72, 73, 84, 85, 87, 91, 96, 98]\n"
          ]
        }
      ],
      "source": [
        "# step 4\n",
        "from scipy import stats\n",
        "\n",
        "tumourSizeData = data[:, 0]\n",
        "ageData = data[:, 1]\n",
        "cancer = data[:, 2]\n",
        "tumourSizeMean = tumourSizeData.mean()\n",
        "tumourSizeStd = tumourSizeData.std()\n",
        "\n",
        "ageMean = ageData.mean()\n",
        "ageStd = ageData.std()\n",
        "\n",
        "tumourGivenCancerData = tumourSizeData[cancer == 1]\n",
        "ageGivenCancerData = ageData[cancer == 1]\n",
        "tumourGivenNoCancerData = tumourSizeData[cancer == 0]\n",
        "ageGivenNoCancerData = ageData[cancer == 0]\n",
        "def var_name(var, all_vars = locals()):\n",
        "  result = [var_name for var_name in all_vars if all_vars[var_name] is var]\n",
        "  return result[0] \n",
        "\n",
        "# print each class mean and standard deviation\n",
        "for specificData in [tumourGivenCancerData, ageGivenCancerData, tumourGivenNoCancerData, ageGivenNoCancerData]:\n",
        "    mean = specificData.mean()\n",
        "    std = specificData.std()\n",
        "    name = var_name(specificData).replace('Data','')\n",
        "    print('P_{} mean: {}, standard deviation:{}'.format(name, round(mean, 2), round(std, 2)))\n",
        "\n",
        "# using Gaussian Distribution to calculate the specific data\n",
        "def calOneColumnProbability(value, specificData):\n",
        "  mean = specificData.mean()\n",
        "  std = specificData.std()\n",
        "  probability = stats.norm.pdf(value, mean, std)\n",
        "  return probability\n",
        "\n",
        "\n",
        "# the probability of having specific tumour size and age, given patients having cancer\n",
        "def calP_TumourAgeGivenCancer(size, age):\n",
        "  P_TumourGivenCancer = calOneColumnProbability(size, tumourGivenCancerData)\n",
        "  P_AgeGivenCancer = calOneColumnProbability(age, ageGivenCancerData)\n",
        "  return P_TumourGivenCancer * P_AgeGivenCancer\n",
        "\n",
        "\n",
        "# the probability of having specific tumour size and age, given patients not having cancer\n",
        "def calP_TumourAgeGivenNoCancer(size, age):\n",
        "  P_tumourGivenNoCancer = calOneColumnProbability(size, tumourGivenNoCancerData)\n",
        "  P_ageGivenNoCancer = calOneColumnProbability(age, ageGivenNoCancerData)\n",
        "  return P_tumourGivenNoCancer * P_ageGivenNoCancer\n",
        "\n",
        "\n",
        "# the probability of having cancer given tumour size and age\n",
        "def calP_CancerGivenTumourAge(size, age):\n",
        "  P_Cancer = len(cancer[cancer == 1])/len(cancer)\n",
        "  P_NoCancer = 1 - P_Cancer\n",
        "  P_TumourAgeGivenCancer = calP_TumourAgeGivenCancer(size, age)\n",
        "  P_TumourAgeGivenNoCancer = calP_TumourAgeGivenNoCancer(size, age)\n",
        "  return P_TumourAgeGivenCancer * P_Cancer / (P_TumourAgeGivenCancer * P_Cancer + P_TumourAgeGivenNoCancer * P_NoCancer)\n",
        "\n",
        "# calculate accuracy and incorrect indexes\n",
        "def cal_accuracy_and_incorrect_indexes():\n",
        "  right_num = 0\n",
        "  incorrect_indexes = []\n",
        "  for i, item in enumerate(data):\n",
        "    probability = calP_CancerGivenTumourAge(item[0], item[1])\n",
        "    if (probability > .5 and item[2] == 1) or (probability < .5 and item[2] == 0):\n",
        "      right_num += 1\n",
        "    else:\n",
        "      incorrect_indexes.append(i)\n",
        "  return right_num / len(data), incorrect_indexes\n",
        "\n",
        "bayesAccuracy, incorrect_indexes = cal_accuracy_and_incorrect_indexes()\n",
        "print('accuracy:',bayesAccuracy)\n",
        "print('incorrect indexes:', incorrect_indexes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "algorithm_assignment1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
