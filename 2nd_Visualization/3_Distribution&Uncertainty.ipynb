{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Probability Distributions\n",
    "\n",
    "formula: $P(X=x) \\text{ or }P(x)$\n",
    ", where $X$ is variable, $x$ is value.\n",
    "\n",
    "**Probability Mass Function (PMF)**\n",
    "$$p(x)\\text{, where }\\sum_{x}p(x) = 1\\tag{1}$$\n",
    "\n",
    "**Join Probability**\n",
    "\n",
    "Consider two random variables: C, T, and joint probability is \n",
    "$$p(c, t)\\text{, and sum: }\\sum_c\\sum_t{p(c,t)} = 1 \\tag{2}$$\n",
    "\n",
    "|$p(c,t)$|$T=\\text{true}$|$T=\\text{false}$|$p(c)$|\n",
    "|--|--|--|--|\n",
    "|$C=\\text{true}$||||\n",
    "|$C=\\text{false}$||||\n",
    "|$p(t)$||||\n",
    "\n",
    "\n",
    "**Marginalisation**\n",
    "\n",
    "$$p(t) = \\sum_c{p(t, c)}\\tag{3}$$\n",
    "\n",
    "**Conditional Probability**\n",
    "\n",
    "$$p(c|t) = \\frac{p(c,t)}{p(t)}\\tag{4}$$\n",
    "\n",
    "from fromula (4), we can derive:\n",
    "1. **Product Rule**:\n",
    "$$p(c,t) = p(c|t)p(t)\\tag{4.1}$$\n",
    "2. **Bayes Rule**:\n",
    "$$p(c|t)=\\frac{p(t|c)p(c)}{p(t)}\\tag{4.2}$$\n",
    "\n",
    "**From above, if it derives everything from $p(t,c)$.**\n",
    "\n",
    "Then: Marginalization is formula (3), Conditional probability is formula (4), Product rule is (4.1) and Bayes rule is (4.2).\n",
    "\n",
    "**If it derives everything from $p(t,c|X=true)$.**\n",
    "1. Marginalization: $p(c|t,x)$\n",
    "2. Cond. Prob: $p(c|t,x)=\\frac{p(c,t|x)}{p(t|x)}$\n",
    "3. Product Rule: $p(c,t|x) = p(c|t,x) p(t|x)$\n",
    "4. Bayes Rule: $p(c|t,x) = \\frac{p(t|c,x) p(c|x)}{p(t|x)}$\n",
    "\n",
    "**Two random variables X and Y are independent** iff:<br/>\n",
    "$p(x|y)=p(x), p(y|x)=p(y), p(x,y)=p(x)p(y)$, for all $x \\text{ and } y$.<br/>\n",
    "Written: $X  {\\perp\\!\\!\\!\\!\\perp} Y$\n",
    "\n",
    "**X and T are conditionally independent give C** iif:<br/>\n",
    "$p(x|t,c) = p(x|c), p(t|x,c)=p(t|c), p(x,t|c)=p(x|c)p(t|c)$, for all $x, y\\text{ and }c$.<br/>\n",
    "Written: $X {\\perp\\!\\!\\!\\!\\perp} Y \\text{ | } C$\n",
    "\n",
    "\n",
    "**Independence**: a property of a joint distribution.\n",
    "* One variables does not carry information about other variable.\n",
    "\n",
    "**Conditionally independence**: also a property of a joint distribution.\n",
    "* One variable does not carry information about other variable, provided a third variable is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Distributions\n",
    " \n",
    "Empirical Cumulative Distribution Function (**ECDF**), based on sample observation:\n",
    "$$\\hat{F}(x) = \\frac{1}{n}\\sum^{n}_{i=1}1(x_i\\leq x)\\tag{5}$$\n",
    "\n",
    "Continuous Probability Distributions (**CDF**):\n",
    "$$F(x) = \\lim_{n \\rightarrow \\infty}\\frac{1}{n}\\sum^{n}_{i=1}1(x_i\\leq x)\\tag{6}$$\n",
    "Its features:\n",
    "* $0<F(x)<1$\n",
    "* Probability that $x_i < x$\n",
    "* Monotonically non-decreasing\n",
    "* Steep slope -> large sample density\n",
    "* completely describes distribution\n",
    "\n",
    "Probability Density Function (**PDF**):\n",
    "$$f(x) = \\frac{dF(x)}{dx}\\text{  or  }p(x) = \\frac{dF(x)}{dx}\\tag{7}$$\n",
    "\n",
    "\n",
    "Particular shape of PDF: **The Normal Distribution**\n",
    "$$f(x;\\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\tag{8}$$\n",
    ", which can used many case as many things in natures are approximately normal distributed. And the central limit theorem. We can estimate mean and standard deviation:\n",
    "$$\\hat{\\mu} = \\frac{1}{n}\\sum^{n}_{i=1}x_i\\tag{9.1}$$\n",
    "$$\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum^{n}_{i=1}(x_i - \\hat{\\mu})^2}\\tag{9.2}$$\n",
    "\n",
    "**Visualise continuous probability distributions**\n",
    "1. **Histogram**, where $\\int_{-\\infty}^{+\\infty} \\hat{f}(x)d(x) = 1$\n",
    "2. **Kernel Density Estimator(KDE)**: It place one Gaussian at each datapoint, can approximate PDF as $\\hat{f}(x) = \\frac{1}{n}\\sum_{i=1}^{n}K(x,x_i;\\sigma_k)$, $\\sigma_k$ is bandwidth which affects graph a lot.\n",
    "3. Compare Multiple distributions:\n",
    "    1. **Box plot** can bed used to compare multiple distributions. The features are:\n",
    "        * Outliers\n",
    "        * Maximum\n",
    "        * $3^{\\text{rd}}$ Quartile (75%)\n",
    "        * Median (50%)\n",
    "        * $1^{\\text{st}}$ Quartile (25%)\n",
    "        * Minimum\n",
    "    4. **Strip plot, Swarm plot, Violin plot** can compare it in different ways\n",
    "4. Joint Continuous Distributions\n",
    "    * joint distribution in scatter plot, and bins plot\n",
    "\n",
    "Above rules also apply in continuous distribution, just change PMF into PDF:\n",
    "* Marginalisation: $p(t) = \\int_{-\\infty}^{+\\infty}p(t,c)dc$\n",
    "* Conditionality Probability: $p(c|t) = \\frac{p(c,t)}{p(t)}$\n",
    "* Product rule: $p(c,t) = p(c|t)p(t)$\n",
    "* Bayes rule: $p(c|t) = \\frac{p(t|c)p(c)}{p(t)}$\n",
    "\n",
    "Looking back to **Normal distribution**:\n",
    "Formula is in (8). If $\\mu=0$ and $\\sigma=1$, it is **Standard Normal Distribution**. So the rule of three names is: $\\text{StandardNormals} \\subset{\\text{Nomal (Gaussian) distribution}}$.\n",
    "\n",
    "\n",
    "The product of two gaussian functions a, b, which new mean $\\mu$ and standard deviation $\\sigma$ will be:\n",
    "$$\\mu = \\frac{\\mu_a\\sigma_b^2 + \\mu_b\\sigma_a^2}{\\sigma_a^2 + \\sigma_b^2} \\tag{11.1}$$\n",
    "$$\\sigma = \\sqrt{\\frac{\\sigma_a^2\\sigma_b^2}{\\sigma_a^2 + \\sigma_b^2}}\\tag{11.2}$$\n",
    "\n",
    "Fourier transfrom of a Gaussian still is a Gaussian :\n",
    "|In time domain|In frequency domain|\n",
    "|--|--|\n",
    "|$\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(-\\frac{t^2}{2\\sigma^2})$|$\\exp(-\\frac{f^2}{2\\sigma_f^2}), \\sigma_f = \\frac{1}{\\sigma2\\pi}$|\n",
    "\n",
    "When $\\sigma$ is larger, function will narrower in frequency domain.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncertainty\n",
    "\n",
    "For visualizing uncertaity, except distribution, mean and standrad deviation, what we really need is visualizing:\n",
    "* How sure about our mean estimation.\n",
    "* Does it reflect increased certainty from multiple measurements.\n",
    "\n",
    "\n",
    "### Intervals\n",
    "\n",
    "**Intervals in the Normal distribution**\n",
    "\n",
    "We can use Quantile Function $F^{-1}(C)$, aka Percent-Point-Function (PPF) to find the threshold proportion.\n",
    "\n",
    "Intervals $[a,b]$:\n",
    "$$a=\\mu - z\\sigma\\\\b=\\mu + z\\sigma\\\\z=-F^{-1}(\\frac{1-C}{2}) \\tag{12}$$\n",
    "\n",
    "Regularly Used $z$ values for double sided interval:\n",
    "\n",
    "|$z$|Percentage|\n",
    "|--|--|\n",
    "|3|99.7%|\n",
    "|2.576|99%|\n",
    "|2|95.4%|\n",
    "|1.96|95%|\n",
    "|1.645|90%|\n",
    "|1|68.3%|\n",
    "\n",
    "#### Calculate Posterior\n",
    "There we assume: we have known below conditions\n",
    "1. **Prior**: $p(u) = \\frac{1}{\\sigma_p\\sqrt{2\\pi}}\\exp(-\\frac{(\\mu-\\mu_p)^2}{2\\sigma_p^2})$\n",
    "2. **sample**: $\\bf{x}=(x_1,x_2,...,x_n)$\n",
    "3. sample mean $\\bar{x}=\\frac{1}{n}\\sum_{1}^{n}x_i$ \n",
    "4. sample std: $\\bar{\\sigma}$\n",
    "\n",
    "For the **likelihood**:\n",
    "$$p(\\bar{x}|\\mu)=\\frac{1}{\\bar{\\sigma}\\sqrt{2\\pi}}\\exp(-\\frac{(\\bar{x} - \\mu)^2}{2\\bar{\\sigma}^2})$$\n",
    "For the **Posterior**:\n",
    "$$p(\\mu|x) \\propto p(\\bar{x}|\\mu)p(u)\\tag{13}$$\n",
    "We can use *formula (11.1)* and *formula (11.2)* to get the approximate new normal distribution posterior.\n",
    "\n",
    "$$\\mu_{post} = \\frac{\\mu_p\\bar{\\sigma}^2 + \\bar{\\mu}\\sigma_p^2}{\\sigma_p^2 + \\bar{\\sigma}^2} \\tag{13.1}$$\n",
    "$$\\sigma_{post} = \\sqrt{\\frac{\\sigma_p^2\\bar{\\sigma}^2}{\\sigma_p^2 + \\bar{\\sigma}^2}}\\tag{13.2}$$\n",
    "\n",
    "\n",
    "\n",
    "#### Credible Intervals\n",
    "There are **two steps** need to finish:\n",
    "1. Compute the Posterior\n",
    "2. Compute the interval:\n",
    "$$[a=\\mu_{pot} - z\\sigma_{post}, b=\\mu_{post}+z\\sigma_{post}]\\tag{14}$$\n",
    "\n",
    "\n",
    "**Interpretation** of credible intervals: Give the observed data $\\bf{x}$, there is a 95% probability that the true value lies between $a$ and $b$.\n",
    "\n",
    "\n",
    "**Limitations**:\n",
    "1. If the distributions are not Gaussian, things get more complicated.\n",
    "2. If the standard deviation $\\sigma$ is unknown, we also need a prior of the standard deviation.\n",
    "3. If we can not know the prior, it would rise to a difficult philosophical question.  :(\n",
    "\n",
    "\n",
    "#### Confidence Interval\n",
    "\n",
    "**Interpretation**: No matter what the true value $\\mu$ is, we have 95% chance of drawing a sample $\\bf{x}$ so that we end up with an interval that contains $\\mu$.\n",
    "\n",
    "**Comparison**\n",
    "\n",
    "|Credible Interval|Confidence Interval|\n",
    "|--|--|\n",
    "|Easier to interpret|Easer to compute|\n",
    "|Requires prior|Requires no prior|\n",
    "|Will be potentially tighter|Will be wider|\n",
    "|Compromise between data and prior|Centred at sample mean|"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
