{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "**Why do we need reduce dimensionality?**<br/>\n",
    "There exist the phenomenon known as the **curse of dimensionality (CoD)**: When the dimension increases, the number of samples should increase exponentially in order for the model to do equally well as in low dimensional spaces.\n",
    "The  advantages of dimensionality reduction:\n",
    "1. Avoiding overfitting: as the more dimensions exist, the more complex model will be.\n",
    "2. Improve accuracy.\n",
    "3. Less computing so that algorithms can train faster.\n",
    "4. Less storage space required.\n",
    "5. Less redundant features and noise.\n",
    "6. Yield a more compact, more easily interpretable representation of the target concept, focusing the user's attention on the most relevant dimensions.\n",
    "\n",
    "**Why do we still need high dimensions?**<br/>\n",
    "There exists **the bless of dimensionality**: In high dimensions, data are easier to be classified or graphed. We need to use high dimensions to know the relations between data.\n",
    "\n",
    "### Mathematical concepts\n",
    "#### Positive (semi-)definite matrices\n",
    "The matrix $\\bf{A}$ is positive semi-definite if it can be obtained as:\n",
    "$${\\bf A}=XX^T\\tag{1}$$\n",
    "\n",
    "Property: Its eigenvalues are always positive or null.\n",
    "\n",
    "Another definition for matrix $\\bf{A}$ with any non-zero vector $\\bf{x}$:<br/>\n",
    "* positive semi-definite: $$\\bf{x^T}\\bf{A}\\bf{x}  \\geq 0\\quad\\forall \\bf{x} \\tag{2}$$\n",
    "* negative semi-definite: above relationship is $\\leq0$.\n",
    "* positive definite: above relationship is $\\gt 0$\n",
    "\n",
    "#### Trace\n",
    "$trace\\{\\bf{A}\\}$ equal to the sum of its diagonal elements, also equal to the sum its eigenvalues $\\sum_{l}\\lambda_{l}$\n",
    "$$\\text{trace}\\{A\\} = \\sum_{l}\\lambda_{l} = \\text{trace}\\{\\Lambda\\}\\tag{3}$$\n",
    "\n",
    "#### Determinant\n",
    "$$|\\bf{A}| = \\prod_l\\lambda_l \\quad\\text{with }\\lambda_l \\text{ being the }l\\text{-th eigenvalue of } \\bf{A}\\tag{4}$$\n",
    "\n",
    "#### Rank\n",
    "The number of non-zero eigenvalues of the matrix.\n",
    "$$rank\\{\\bf{A}\\}\\tag{5}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct PCA\n",
    "\n",
    "for each datapoint:<br/>\n",
    "Raw: $\\bf x \\in R^d$<br/>\n",
    "Target: $\\bf y \\in R^p$, where $p<<d$.\n",
    "\n",
    "We have matrix $X$ to represent all the data. $X=[\\bf x_1, ..., x_n]_{d \\times n}\\quad x_i\\in R^d$\n",
    "\n",
    "Because when we decrease dimension, the new fewer dimensions we should keep more variance. Fo new coordinate directions (eigenvectors) ${\\bf u}_{d\\times 1}$, our target is:\n",
    "$${{\\bf u}^T} X \\rarr \\max_{{\\bf u}} Var({{\\bf u}^T}X)$$\n",
    "\n",
    "Mean: $\\bar X = [\n",
    "    {\\frac{1}{n}\\sum_{i=1}^{n}{\\bf x_i}}, \n",
    "    \\frac{1}{n}\\sum_{i=1}^{n}{\\bf x_i},\n",
    "    ...,\n",
    "    \\frac{1}{n}\\sum_{i=1}^{n}{\\bf x_i}\n",
    "]_{d \\times n}$\n",
    "\n",
    "Co-variance: $S=(X-\\bar X)(X - \\bar X)^T$<br/>\n",
    "Variance in production: $$\\begin{aligned}\n",
    "\\max_{\\bf u} Var({\\bf u}^TX)\n",
    "&= \\frac{1}{n}({\\bf u}^TX - {\\bf u}^T\\bar{X})^2 \\\\\n",
    "&=\\frac{1}{n}{\\bf u}^T (X-\\bar X)(X - \\bar X)^T{\\bf u} \\\\\n",
    "&=\\frac1n {\\bf u}^TS{\\bf u} \\quad \\text{s.t.}\\ \\ {\\bf u}^T{\\bf u}=I\n",
    "\\end{aligned}$$\n",
    "\n",
    "Use **Lagrange multipers** to find the relationships in maximum value.\n",
    "$$L({\\bf u}, \\lambda) = {\\bf u}^TS{\\bf u} - \\lambda({\\bf u}^T{\\bf u} - I)$$\n",
    "Then\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial {\\bf u}} &= 2S{\\bf u} - 2\\lambda {\\bf u} = 0\\\\\n",
    "&\\Rightarrow S{\\bf u} = \\lambda {\\bf u}\n",
    "\\end{aligned}$$\n",
    "so\n",
    "$$\\max_{\\bf u} Var({\\bf u}^TX)=\\frac1n{\\bf u}^TS{\\bf u}=\\frac1n{\\bf u}^T\\lambda {\\bf u}=\\lambda {\\bf u}^T{\\bf u} = \\lambda$$\n",
    "\n",
    "As $S$ at most has d eigenvalues, we cant sort in a descending order, to get first p maximum eigenvalues and corresponding eigenvectors.\n",
    "\n",
    "Above equation can be also written in this format:\n",
    "$$SU = \\Lambda U$$\n",
    "$\\Lambda= [\\lambda_1, \\lambda_2, ..., \\lambda_d]$ is Eigenvalues (have at most d numbers) with diagonal matrix, which corresponding eigenvectors are $U_{d\\times d}  = [{\\bf u_1, u_2, ... u_d}]$\n",
    "\n",
    "From above, the steps to do PCA are:\n",
    "1. Compute co-variance matrix $S=(X-\\bar X)(X-\\bar X)^T$ from give data $X\\in R^{d\\times n}$\n",
    "2. Compute eigenvectors $U_{d\\times d}$, and eigenvalues $\\Lambda_{d\\times d}$.\n",
    "3. Reduce data dimensions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "For dataset $X \\in R^{d\\times n}$, we have\n",
    "\n",
    "$$X = U \\Sigma V^T$$\n",
    "\n",
    "* $U_{d \\times d}$: eigenvectors of $XX^T$\n",
    "* $\\Sigma_{d \\times n}$: diagonal matrix, where $\\Sigma_1 = \\sqrt \\lambda_1, \\Sigma_2 = \\sqrt \\lambda_2, ...$, and $\\Sigma_1 > \\Sigma_2 > ...$\n",
    "* $V_{n \\times n}$: eigenvectors of $X^TX$\n",
    "\n",
    "From above, if we use SVD to compute sorted eigenvectors and eigenvalues for PCA, the steps are:\n",
    "1. Remove mean $\\bar X$ from data $X$: $X - \\bar X$\n",
    "2. Apply SVD to $X$: $X - \\bar X$ to compute $U$ and the first p column vectors which are first p Principal Components (PCs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual PCA\n",
    "\n",
    "If $d>>n$, it is not efficient using direct PCA to compute eigenvectors (pcs).\n",
    "\n",
    "In this case, using dual PCA by SVD can solve it faster.\n",
    "\n",
    "From the PCA equation, we should replace the $U$ eigenvectors of $XX^T$ to make it calculate faster.\n",
    "combine SVD, equation, \n",
    "$$Y = U^TX = U^TU\\Sigma V^T=\\Sigma V^T$$\n",
    "And\n",
    "$$\\begin{aligned}\n",
    "X &= U \\Sigma V^T\\\\\n",
    "XV&= U \\Sigma V^T V\\\\\n",
    "XV\\Sigma^{-1} &= U \\Sigma V^T V\\Sigma^{-1}\\\\\n",
    "\\Rightarrow U &= XV\\Sigma^{-1} \n",
    "\\end{aligned}$$\n",
    "\n",
    "So for the dual PCA:\n",
    "* **Recover PCs**: ① Calculate $X^TX$ and let V = eigenvectors of $X^TX$. ② Let $\\Sigma$ = diagonal matrix of square roots of the p eigenvalues. Note that $X$ has been centralised.\n",
    "* **Project training data**: $Y_{p\\times n} = U^T X = \\Sigma V^T$\n",
    "* **Recover training data**: $\\tilde X = UY = U\\Sigma V^T = XV\\Sigma^{-1}\\Sigma V^T = XVV^T$ \n",
    "* **Project one of the test data**: ${\\bf y} = U^T {\\bf x} = (XV\\Sigma^{-1})^T{\\bf x} = \\Sigma^{-1}V^TX^T{\\bf x}$\n",
    "* **Recover one of the test data** $\\tilde x = U{\\bf y} = UU^T{\\bf x} = (XV\\Sigma^{-1})(XV\\Sigma^{-1})^T{\\bf x} = XV\\Sigma^{-2}V^TX^T{\\bf x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to know how many dimensions should keep\n",
    "\n",
    "1. keep specified variance to the original dataset $X$\n",
    "$$\\frac{\\sum_{i=1}^p \\lambda_i}{\\text{Trace}\\{S\\}}$$\n",
    "\n",
    "2. Scree plot, find the \"elbow\" of the graph, the principal components to left of the point is retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA\n",
    "\n",
    "Direct PCA and dual PCA work perfectly for linear datset to find the subspace. But for the nonlinear dataset where exist manifold data, these two method can not work.\n",
    "\n",
    "So **the idea of the kernel PCA** is: take data to a higher dimensional space such that data becomes linearly distributed in that high dimension space. The do either classification or dimensionality reduction using linear methods.\n",
    "\n",
    "Change data in this way:\n",
    "$${X^TX}_{n \\times n} \\rightarrow \\Phi^T(X)\\Phi(X) = {K(X,X)}_{n \\times n}$$\n",
    "Kernel trick: Don't care about $\\Phi$, but $\\Phi^T\\Phi$ which can be computed from a kernel funciton $K(X,X)$.\n",
    "\n",
    "The Kernel PCA methods are (the similar way in Dual PCA):\n",
    "* **Recover PCs**: ① Calculate $\\Phi^T(X)$ using kernel K and let $V_{}$ = eigenvectors of K matrix corresponding to the top $p$ eigenvalues. ② Let $\\Sigma$ = diagonal matrix of square roots of the top $p$ eigenvalues.\n",
    "* **Project training data**: $Y_{p\\times n} = U^T\\Phi(X) = \\Sigma V^T$\n",
    "* Recover Training data: $\\tilde{X} = UY = \\Phi(X)VV(T)$, $\\Phi(X)$ is unknown so recovery can not be done.\n",
    "* **Project one of the test data**: ${\\bf y} = U^T\\Phi({\\bf x}) = (\\Phi(X)V\\Sigma^{-1})^T\\Phi({\\bf x}) = \\Sigma^{-1}V^T\\Phi(X)\\Phi(x)$, $\\Phi(X)\\Phi(x) = K(X, x)$ is computable, so the result can be calculated.\n",
    "* Recover test data can not be done as $\\Phi(x)$ is unknown.\n",
    "\n",
    "Commonly used kernels:\n",
    "1. Linear: $K_{ij} = < {\\bf x_i}, {\\bf x_j}>$\n",
    "2. Polynomial: $K_{ij} = (1 + <{\\bf x_i}, {\\bf x_j}>)^P$\n",
    "3. Gaussian: $K_{ij} = e^{-r||{\\bf x_i} - {\\bf x_j}||_2^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS & Isomap\n",
    "\n",
    "Assumption: two close data points in the original high-dimensional space should be also close after they are projected to a low-dimensional space.\n",
    "\n",
    "Steps to find MDS solution: give data $X=[{\\bf x_1},..., {\\bf x_n}]$\n",
    "1. Compute the pair-wise distance matrix $D^{(x)}$.\n",
    "2. Using the centering matrix $H = I_{n\\times n} - \\frac1n ee^T\\text{, }\\quad e = [{\\bf 1}]_{n\\times 1}$\n",
    "to centralize $D^{(x)}$: $K = -\\frac12HD^{(x)}H$.\n",
    "3. Determine the p-largest eigenvalues $\\Lambda_{p\\times p}$: $Diag\\{\\lambda_1, \\lambda_2, ...\\lambda_p\\}$ and the corresponding eigenvalues $V_{n\\times p}$: $[{\\bf u_1},{\\bf u_2},...,{\\bf u_p} ]$ of $K$.\n",
    "4. project data: $Y = \\Sigma V^T =\\Lambda^{\\frac12}V^T$\n",
    "\n",
    "Dual PCA is same as MDS when $D^{(x)}$ is Eucliean distance:\n",
    "$$K = -\\frac12HD^{(x)}H = X^TX$$\n",
    "\n",
    "MDS is more general as $D^{(x)}$ can be other distance matrices:\n",
    "1. geodesic distance\n",
    "2. hamming distance<br/>\n",
    "...<br/>\n",
    "\n",
    "How to choose distance matrix is \"art\" of data science problem driven, If $D^{(x)}$ is not Eculidean, MDS is not dual PCA anymore.\n",
    "\n",
    "\n",
    "#### Isomap\n",
    "\n",
    "in Isomap: $K = -\\frac12HD^{(G)}H$, $D^{(G)}$: Geodesic distance\n",
    "\n",
    "Steps:\n",
    "1.  define the graph $G$ over all data points using K nearest neighbors, based on Euclidean distance.\n",
    "2. Compute the shortest path between all points as an estimation of geodesic distance $D^{(G)}$. (Algorithms: Dijkstra or Floyd)<br/>\n",
    "...<br/>\n",
    "The following steps are consistent with MDS.\n",
    "\n",
    "**Note**: $K$ with geodesic distance may not be positive semi-definite matrix. Therefore, eigenvalues $\\Lambda_p$ may have negative values. We still can project data using only eigenvectors corresponding to positive eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Linear Embedding (LLE)\n",
    "\n",
    "**Intuition of LLE**: Local structures in high-dimensional space should be retained in low-dimensional space after projection (embedding).\n",
    "\n",
    "Three steps:\n",
    "\n",
    "#### 1. Select neighbours\n",
    "Find a set of the nearest neighbours of each point in the dataset\n",
    "\n",
    "Construct a K-nearest neighbour graph $N_i, i \\in \\{1, ..., n\\}$\n",
    "\n",
    "#### 2. Reconstruct with linear weights\n",
    "Compute a set of weights (similarities) for each point that best describes the point as a linear combination of its nearest neighbours.\n",
    "\n",
    "Minimise the first object function\n",
    "$$\\min_{w_{ij}} \\sum_{i=1}^{n} |{\\bf x_i} - \\sum_{j=1}^kw_{ij}{\\bf x}_{N_i(j)} |^2\\quad\\text{s.t.}\\sum_{j=1}^k w_{ij} = 1 $$\n",
    "$N_i(j)$ is the index of $j^{th}$ nearest neighbour of $x_i$.\n",
    "\n",
    "What happen here? we can say that for each datapoint, it is equivalent to sum of its neighbor points with weight. and the weights is subject to sum is 1.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x_i = w_{i1}{\\bf x}_{N_{i}(1)} + w_{i2}{\\bf x}_{N_{i}(2)} + w_{i3}{\\bf x}_{N_{i}(3)} + ... + w_{ik}{\\bf x}_{N_{i}(k)} \\\\\n",
    "\\text{s.t.}\\quad w_{i1} + w_{i2} + w_{i3} + ... + w_{ik} = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Why do we need subject to** ${\\bf e^T w}_i = 1$? As in this way, the equation can be transform to which can be used Lagrange multiplier to solve it easily and not other operations.\n",
    "\n",
    "Details:\n",
    "\n",
    "For the one of the datapoints ${\\bf x}_i$, we define:\n",
    "$$\\begin{aligned}\n",
    "N_i &= [{\\bf x}_{N_{i}(1)}, {\\bf x}_{N_{i}(2)}, ... ,{\\bf x}_{N_{i}(k)}]_{d\\times k}\\\\\n",
    "{\\bf w_i} &= [w_{i1}, w_{i2}, ..., w_{ik}]_{k\\times 1}\\\\\n",
    "{\\bf e} &= [1, 1, ..., 1]_{k\\times 1}\n",
    "\\end{aligned}$$\n",
    "The equation can be transformed:\n",
    "$$\\begin{aligned}\n",
    "&|{\\bf x}_i - \\sum_{j=1}^kw_{ij}{\\bf x}_{N_i(j)}|^2\\\\\n",
    "=& | {\\bf x}_i {\\bf e}^T {\\bf w}_i - N_i {\\bf w}_i |^2\\\\\n",
    "=& | ({\\bf x}_i {\\bf e}^T - N_i) {\\bf w}_i|^2\\\\\n",
    "=& [({\\bf x}_i {\\bf e}^T - N_i) {\\bf w}_i]^T[({\\bf x}_i {\\bf e}^T - N_i) {\\bf w}_i]\\\\\n",
    "=& {\\bf w}_i^T({\\bf x}_i {\\bf e}^T - N_i)^T({\\bf x}_i {\\bf e}^T - N_i){\\bf w}_i\\\\\n",
    "=& {\\bf w}_i^T G_i{\\bf w}_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "Where $G_i$ is Gram matrix $_{k\\times k}$, is a positive semi-definite, so we can use Lagrange multiplier to get the optimal weights.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "L({\\bf w}_i, \\lambda) &= {\\bf w}_i^T G_i {\\bf w}_i - \\lambda ({\\bf e}^T{\\bf w}_i - 1)\\\\\n",
    "\\frac{\\partial L}{\\partial {\\bf w}_i} &=2G_i{\\bf w}_i - \\lambda {\\bf e} = 0\\\\\n",
    "\\end{aligned}\\\\\n",
    "G_i {\\bf w}_i = \\frac{\\lambda}{2}{\\bf e}$$\n",
    "\n",
    "If $\\lambda$ is known, we can compute ${\\bf w}_i$. $\\lambda$ can be set any arbitrary positive value, then normalise ${\\bf w}_i$ such that sum is 1.\n",
    "\n",
    "Note: $G_i$ is not always invertible, we can add small number to avoid this case:\n",
    "$${\\bf w}_i = \\frac{\\lambda}2(G_i + \\Sigma I)^{-1} {\\bf e}\\quad \\forall i \\in [1,...,n]$$\n",
    "\n",
    "\n",
    "#### 3. Map to embedded coordinates\n",
    "Use eigen-decomposition to find the low-dimensional embedding of these high-dimensional, such that each point is still described with the same linear combination of these high-dimensional.\n",
    "\n",
    "Compute the embedding $Y \\in R^{p\\times n}$ using $w_{ij}$ by minimising the second objective function, we asume we know the $Y$ to do it:\n",
    "$$\\min_{Y} \\sum_{i=1}^n |{\\bf y}_i - \\sum_{j=1}^n w_{ij}{\\bf y}_{N_i(j)}|^2\\\\\n",
    "\\text{s.t.}\\quad YY^T = 1, \\sum_{i=1}^n y_i = 0\n",
    "$$\n",
    "\n",
    "constrains explain:\n",
    "1. $YY^T = 1$: ① To avoid trivial solution, which will give you 0 value, it is not we want, so we put this constrain to make sure we have solution. ② Control the length of vector ${\\bf y}_i$, only need unit length, like the direct PCA.\n",
    "2. $\\sum_{i=1}^n y_i = 0$: It is like centralise the projected value as LLE have not idea do the centralisation first. (target value is eigenvalue, which is optimisation problem now.).\n",
    "\n",
    "There we define:\n",
    "$$\n",
    "I = \\text{diag}\\{1\\}_{n\\times n}\\quad W = [{\\bf w}_1, {\\bf w}_2, ..., {\\bf w}_i]_{n\\times n}\\quad w_i = [0,0,...,w_{i1}...w_{ik},...,0]$$\n",
    ", $w_{i1}...w_{ik}$ are weights of K-nearest neighbours of point, 0 otherwise.\n",
    "\n",
    "The second objective function can be written:\n",
    "$$\\begin{aligned}\n",
    "    \\min_{Y}& \\sum_{i=1}^n [{\\bf y}_i^2 + (\\sum_{j=1}^n w_{ij}{\\bf y}_{N_i(j)})^2 - 2{\\bf y}_i \\sum_{j=1}^n {w_{ij}{\\bf y}_{N_i(j)}}]\\\\\n",
    "=& [{\\sum_{i=1}^n}{\\bf y}_i^2 + {\\sum_{i=1}^n}(\\sum_{j=1}^n w_{ij}{\\bf y}_{N_i(j)})^2 - 2{\\sum_{i=1}^n}{\\bf y}_i \\sum_{j=1}^n {w_{ij}{\\bf y}_{N_i(j)}}]\\\\\n",
    "=& YY^T + YW(YW)^T - 2YWY^T\\\\\n",
    "=& YY^T + YWW^TY^T - YWY^T - YW^TY^T\\\\\n",
    "=& Y(I + WW^T - W - W^T)Y^T\\\\\n",
    "\\Rightarrow \\min_{Y}& Y (I - W)(I -W)^TY^T \\\\\n",
    "=&YMY^T\n",
    "\\end{aligned}$$\n",
    "Apply Lagrange Multipliers:\n",
    "$$\\begin{aligned}\n",
    "L(Y, \\lambda) &= YMY^T - \\lambda(YY^T - I)\\\\\n",
    "\\frac{\\partial L}{\\partial Y} &= MY^T - \\lambda Y^T\\\\\n",
    "MY^T&=\\lambda Y^T\n",
    "\\end{aligned}$$\n",
    "\n",
    "So the equation:\n",
    "$$\\begin{aligned}\n",
    "\\min_Y YMY^T = Y\\lambda Y^T = \\lambda YY^T = \\lambda\n",
    "\\end{aligned}$$\n",
    "\n",
    "$Y^T$ is the eigenvectors corresponding to the smallest eigenvalues $\\lambda$ of $M$:\n",
    "* Find all eigenvectors and eigenvalues of $M$.\n",
    "* Sort eigenvectors using their eigenvalues in an ascending order.\n",
    "* Pick first p+1 eigenvectors\n",
    "* Ignore the first eigenvector which corresponds to 0 eigenvalue. (as the first constrain shows that $YY^T$ has to be 1)\n",
    "* $Y^T_{n\\times p}$ is eigenvectors (2 to p+1), so rows of $Y$ are eigenvectors.\n",
    "\n",
    "Why the minimum of eigenvalue always is 0?\n",
    "We assume: eigenvalue $\\lambda = 0$, and eigenvector $Y^T$ is constant vector ${\\bf a}_{n \\times n}$\n",
    "so\n",
    "$$\\begin{aligned}\n",
    "MY^T &= M{\\bf a}\\\\\n",
    "&=(I - W)(I - W)^T {\\bf a}\\\\\n",
    "&=(I - W)(I^T {\\bf a} - W^T{\\bf a})\\\\\n",
    "&=(I - W)(n{\\bf a} - n{\\bf a})\\\\\n",
    "&={\\bf 0} = \\lambda {\\bf a}\n",
    "\\end{aligned}$$\n",
    "\n",
    "From above, eigenvalues $\\lambda$ always have $0$ value, and **zero eigenvalue always have constant value of eigenvectors.**\n",
    "\n",
    "Why dropping the eigenvalue satisfy the constraint that the outputs have a zero mean (i.e. $\\sum_{i=1}^n {\\bf y}_i = 0$).\n",
    "1. As the first eigenvector which is constant vector is orthogonal to other vectors, so it can make the summation of each vector actually is zero. Below is prove:\n",
    "$$\\begin{aligned}\n",
    "\\text{1st eigenvector: } {\\bf a} &= [a, a, ..., a]_{n\\times 1}\\\\\n",
    "\\text{other eigenvector: } {\\bf v} \n",
    "&= [v_1, v_2, ..., v_n]_{n\\times 1}\\\\\n",
    "{\\bf a}^T \\cdot {\\bf v} &= 0 \\\\\n",
    "\\Rightarrow a (v_1 + v_2 + ... + v_n) &= 0\\\\\n",
    "\\Rightarrow v_1 + v_2 + ... + v_n &= 0\n",
    "\\end{aligned}$$\n",
    "2. And the target value ${\\bf y}_i$ is the eigenvector, which is row-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified framework\n",
    "Summarise methods (dual PCA, kernel PCA, MDS, Isomap and LLE) into an unified framework, where all methods can be viewed as a variant of kernel PCA. \n",
    "\n",
    "#### Kernel PCA\n",
    "There kernel $K_{n\\times n}$ is similarity between each pairwise points.\n",
    "\n",
    "Project train data: $Y = \\Sigma V^T$\n",
    "\n",
    "Project test data: ${\\bf y} = \\Sigma^{-1} V^T K(X, {\\bf x})$\n",
    "#### Dual PCA\n",
    "$K = X^TX$, which is a linear kernel.\n",
    "\n",
    "#### MDS\n",
    "$K=-\\frac12 H{D^2_{(E)}}H$, $D_{(E)}$ is Euclidean distance.\n",
    "\n",
    "#### Isomap\n",
    "$K=-\\frac12 H{D^2_{(G)}}H$, $D_{(E)}$ is Geodesic distance.\n",
    "\n",
    "#### LLE\n",
    "Inverse $M$ which equal to $(I-W)(I-W)^T$ such that it has the same descending order in eigenvalues.\n",
    "$K = M^{-1}$, why it can match the descending order:\n",
    "As $M = U\\Sigma U^T$, and eigenvectors is normalise so we have \n",
    "$$U^{-1}U = I = U^TU\\\\\n",
    "\\Rightarrow U^{-1} = U^T\n",
    "$$\n",
    "Then \n",
    "$$\\begin{aligned}\n",
    "M^{-1} &= (U\\Sigma U^T)^{-1}\\\\\n",
    "&={U^T}^{-1}\\Sigma^{-1}U^{-1}\\\\\n",
    "&={U^T}^T\\Sigma^{-1}U^T\\\\\n",
    "&=U\\Sigma^{-1} U^T\n",
    "\\end{aligned}$$\n",
    "\n",
    "$K = \\Delta_{max} I - M$ also satisfy the condition.\n",
    "\n",
    "In this way,$Y$ consists of eigenvectors corresponding to ${\\cal p}$ largest eigenvalues of K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-Definite Embedding (SDE)\n",
    "\n",
    "Above unified framework shows that all methods can be viewed as a variant of kernel PCA. Which method is optimal depends on which dataset we use as well as how we select hyperparameters. These methods become kernel method so that the kernel matrix is **constructed by maximising the variance of data in the low dimensional space subject to a set of constraints**. The main optimisation used for this maximisation problem is known as semi-definite programming.\n",
    "\n",
    "The idea of SDE is to unfold the manifold, so there are two steps to do:\n",
    "1. Euclidean distance of neighbour points are preserved, so that high dimensional local structures will be retained in low-dimensional space.\n",
    "2. Maximise the pairwise distance of every pair of points: pulling points as far away from each other as possible so this manifold can unfolded.\n",
    "\n",
    "**Step one**:<br/>\n",
    "Data:\n",
    "$$\n",
    "\\text{raw: } X = [{\\bf x}_1, {\\bf x}_2, ..., {\\bf x}_n]_{d\\times n}\\quad \\text{project: } Y = [{\\bf y}_1, {\\bf y}_2, ..., {\\bf y}_n]_{d\\times n}\n",
    "$$\n",
    "\n",
    "After finding the neighbours for each point, use $\\eta$ to check if it is the neighbour of the point.\n",
    "$$\n",
    "\\eta_{ij} = \\begin{cases}\n",
    "1 \\quad\\text{if }i\\text{ is a neighbour of }j\\\\\n",
    "0 \\quad\\text{otherwise}\n",
    "\\end{cases}\\\\\n",
    "$$\n",
    "\n",
    "Reserve the euclidean distance between the point and its neighbours, which also need to be mapped to lower dimensional:\n",
    "$$|{\\bf x}_i - {\\bf x}_j|^2 = |{\\bf y}_i - {\\bf y}_j|^2 \\quad \\text{if }\\eta_{ij} = 1$$\n",
    "\n",
    "If we expand the constraint:\n",
    "$$\\begin{aligned}\n",
    "|{\\bf x}_i - {\\bf x}_j|^2 &= {\\bf y}_i^2 + {\\bf y}_j^2 - 2{\\bf y}_i{\\bf y}_j\\\\\n",
    "&={\\bf y}_i^T{\\bf y}_i + {\\bf y}_j^T{\\bf y}_j - 2{\\bf y}_i{\\bf y}_j\\\\\n",
    "&=K_{ii} + K_{jj} - 2K_{ij}\n",
    "\\end{aligned}$$\n",
    "$K$ is kernel equal to $K = {\\bf y}_i^T{\\bf y}_i$\n",
    "\n",
    "So there are **three constraints** to follow:<br/>\n",
    "1. $K$ is symmetric positive semidefinite, denote by $k\\ge 0$.\n",
    "2. $K$ should be centralised, meaning $\\sum_{ij} K_{ij} = 0 = \\sum_{ij}{\\bf y}_i^T{\\bf y}_j$.\n",
    "3. $K_{ii} + K_{jj} - 2K_{ij} = |{\\bf x}_i - {\\bf x}_j|^2\\quad \\text{if }\\eta_{ij} = 1$\n",
    "\n",
    "Why do we need to do centralise? We want to make K has this properties when after optimising.\n",
    "\n",
    "\n",
    "**Step two**<br/>\n",
    "Maximise the variance of each pairwise distance to unfold the manifold.\n",
    "$$\\begin{aligned}\n",
    "\\max_{Y} &= \\frac12\\sum_{ij}|{\\bf y}_i - {\\bf y}_j|^2\\\\\n",
    "&=\\frac12 (\\sum_i{\\bf y}_i^T{\\bf y}_i + \\sum_i{\\bf y}_j^T{\\bf y}_j - 2\\sum_{ij}{\\bf y}_i^T{\\bf y}_j)\\\\\n",
    "&=\\sum_{i}{\\bf y}_i^T{\\bf y}_i = \\text{Trace}(K)\n",
    "\\end{aligned}$$\n",
    "\n",
    "This objective function and three constraints can have the optimisation, which is call **semi-definite programming**:\n",
    "$$\n",
    "\\max_K \\text{Trace}(K)\\\\\n",
    "\\begin{aligned}\n",
    "\\text{s.t.}\\quad& K\\ge0\\\\\n",
    "&\\sum_{ij}K_{ij}=0\\\\\n",
    "&K_{ii} + K_{jj} - 2K_{ij} = |{\\bf x}_i - {\\bf x}_j|^2\\quad \\text{if }\\eta_{ij} = 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Once we have $K$, we can compute embedding Y from\n",
    "$$Y = \\Sigma V^T$$\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b774acfeff12025bb88a6812344d10674551d33b3b06a1f7dd54b9641286f7ce"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
